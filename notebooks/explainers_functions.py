# -*- coding: utf-8 -*-
"""Explainers_Functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cpQCmUqg3BExxwSxRvHKt9sXvck5Tg-_
"""

from abc import ABC, abstractmethod
from overrides import overrides
import typing
import shap
import torch
from lime.lime_text import LimeTextExplainer
from allennlp.interpret.saliency_interpreters import SimpleGradient
from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer
import numpy as np
from src.data.dataload import *

class Explainer:
    DATASET_LABELS = {
        DatasetSST.NAME: ['3','1','2','4','0'],
        DatasetAGNews.NAME: ['Sports', 'Sci/Tech', 'Business', 'World'],
    }
    
    def __init__(self):
        pass

    @abstractmethod
    def explain_instance(self,s:str)->typing.Any:
      '''
      s - input string
      returns - tokens, weights
      '''
      pass

    @abstractmethod
    def explain_instances(self,S:typing.List[str])->typing.Any:
      '''
      S - list of strings
      returns - tokens, weights
      '''
      pass

class LimeExplainer(Explainer):

  def __init__(self,model):
    labels=Explainer.DATASET_LABELS[model.dataset_finetune.NAME]
    self.exp = LimeTextExplainer(class_names=labels)
    self.tokenizer=model.tokenizer
    self.predict_proba = lambda s: model.predict_proba_batch(s)

  def explain_instance(self,x):
    '''
    x - 1 input instance
    
    returns - list of top tokens/importance weights
    '''

    def predict_probs(x):
        if isinstance(x,str):
            x=[x]

        values=self.predict_proba(x)

        return values
    
    exp_instance=self.exp.explain_instance(x, predict_probs, num_features=50,top_labels=10,num_samples=50)

    pred_label = np.argmax(exp_instance.predict_proba)

    indices=[x[0] for x in exp_instance.as_map()[pred_label]]
    values = [x[1] for x in exp_instance.as_map()[pred_label]]

    return indices,pred_label

  @overrides
  def explain_instances(self,X):
    '''
    X - array of input sentences
    '''

    indices_list=[]
    values_list = []
    pred_list = []

    for s in X:
        try:     
            indices,pred = self.explain_instance(s)

        except:
            indices,pred = ['N/A'],['N/A']

        indices_list.append(indices)
        pred_list.append(pred)

    return indices_list,pred_list

class SHAPExplainer(Explainer):

  def __init__(self, model,tokenizer,labels,device):
    '''
    Currently works only with BERT
    '''
    self.model=model
    self.tokenizer=tokenizer
    self.device=device
    self.predict_proba = lambda x: predict_proba_BERT(x,model,tokenizer,device)
    self.exp=shap.Explainer(self.predict_proba,self.tokenizer,output_names=labels)

  @overrides
  def explain_instances(self,X):
    '''
    X - array of input sentences
    '''

    shap_values = self.exp(X)

    tokens,values=shap_values.data,shap_values.values

    return tokens,values

  @overrides
  def explain_instance(self,x):
    '''
    shap explainer can process lists by default
    '''
    pass

class AllenNLPExplainer(Explainer):

  def __init__(self,model):

    self.model=model
    # self.tokenizer=tokenizer
    # self.device=device
    self.predictor=model.predictor
    self.exp = SimpleGradient(self.predictor)
    
  def explain_instance(self,x):
    '''
    x - 1 input instance
    
    returns - list of top tokens/importance weights
    '''

    explanation = self.exp.saliency_interpret_from_json({"sentence":x})
    grad = explanation['instance_1']['grad_input_1']
    label = self.predictor.predict(x)['label']

    return grad, label

  @overrides
  def explain_instances(self,X):
    '''
    X - array of input sentences
    '''
    grad_list = []
    label_list = []

    for s in X:

      grad, label = self.explain_instance(s)

      grad_list.append(grad)
      label_list.append(label)

    return grad_list, label_list

def predict_proba_BERT(x,model,tokenizer,device):

  '''
  this depends on the model, will be passed to explainer
  '''

  if isinstance(x,str):
    x=[x]

  with torch.no_grad():
    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=128,truncation=True) for v in x]).to(device)
    attention_mask = (tv!=0).type(torch.int64).to(device)
    outputs = model(tv,attention_mask=attention_mask)
    scores = torch.softmax(outputs[0],dim=1)

    return scores.cpu().detach().numpy()

def predict_proba_BCN(x,BCN_predictor):

  #predict only on the sentence
  title = ' '

  a = BCN_predictor.predict_batch_json([
      dict(title=title, sentence=s) for s in x
  ])

  class_probs=np.array([t['class_probabilities'] for t in a])
  return class_probs
