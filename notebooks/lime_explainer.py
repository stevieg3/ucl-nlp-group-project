# -*- coding: utf-8 -*-
"""LIME_explainer.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bs4kFCiGS4OO0Pe-1i9gLEaPEDG0bAni
"""
from lime.lime_text import LimeTextExplainer
import torch
import numpy as np

class Explainer():

  def __init__(self, model,tokenizer,labels,device, explainer_type='LIME',model_type='BERT'):
    '''
    predict_proba - predict function which will depend on model type
    '''
    self.model=model
    self.tokenizer=tokenizer
    self.device=device

    if explainer_type == 'LIME':
      self.exp = LimeTextExplainer(class_names=labels)
      if model_type=='BERT':
        self.predict_proba=lambda x:predict_proba_BERT(x,self.model,self.tokenizer,self.device)
      elif model_type=='BCN':
        self.predict_proba=lambda x:predict_proba_BCN(x,self.model)


  def explain_instance(self,x):
    '''
    x - 1 input instance
    
    returns - list of top tokens/importance weights
    
    '''


    exp_instance=self.exp.explain_instance(x, self.predict_proba, num_features=10,top_labels=5,num_samples=50)

    pred_label = np.argmax(exp_instance.predict_proba)

    top_tokens=[x[0] for x in exp_instance.as_list(label=pred_label)]
    top_values = [x[1] for x in exp_instance.as_list(label=pred_label)]

    return top_tokens,top_values

  def explain_instances(self,X):
    '''
    X - array of input sentences
    '''

    top_tokens_list=[]
    top_values_list = []

    for s in X:

      top_tokens,top_values = self.explain_instance(s)

      top_tokens_list.append(top_tokens)
      top_values_list.append(top_values)

      #print(top_tokens,top_values)

    return top_tokens_list,top_values_list


def predict_proba_BERT(x,bert_model,tokenizer,device):

  '''
  this depends on the model, will be passed to explainer
  '''

  if isinstance(x,str):
    x=[x]

  with torch.no_grad():
    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=128,truncation=True) for v in x]).to(device)
    attention_mask = (tv!=0).type(torch.int64).to(device)
    outputs = bert_model(tv,attention_mask=attention_mask)
    scores = torch.softmax(outputs[0],dim=1)

    return scores.cpu().detach().numpy()

def predict_proba_BCN(x,BCN_predictor):

  #predict only on the sentence
  title = ' '

  a = BCN_predictor.predict_batch_json([
      dict(title=title, Description=s) for s in x
  ])

  class_probs=np.array([t['class_probabilities'] for t in a])
  return class_probs
