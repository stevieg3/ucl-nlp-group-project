{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8848aac",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a7b5b",
   "metadata": {},
   "source": [
    "Notebook for fine-tuning 2 BERT models: 1 for SST and 1 for AGNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1dc7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import spacy\n",
    "from torch.utils.data import \\\n",
    "    TensorDataset, \\\n",
    "    DataLoader\n",
    "from transformers import \\\n",
    "    BertTokenizer, \\\n",
    "    BertForSequenceClassification, \\\n",
    "    AdamW, \\\n",
    "    BertConfig, \\\n",
    "    get_linear_schedule_with_warmup\n",
    "import pytreebank\n",
    "from tqdm import tqdm\n",
    "import shap\n",
    "from checklist.perturb import Perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c07504",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c2da80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataload import load_sst, load_agnews\n",
    "from src.models.bert_utils import \\\n",
    "    pad_sentence_at_end, \\\n",
    "    create_sentence_input_arrays, \\\n",
    "    MAX_LENGTH, \\\n",
    "    BERT_HYPERPARAMETERS, \\\n",
    "    fine_tune_bert, \\\n",
    "    make_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290cc5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3ca64",
   "metadata": {},
   "source": [
    "## SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32df6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = load_sst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337fec4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8544, 2), (1101, 2), (2210, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sst, dev_sst, test_sst = sst.train_val_test\n",
    "train_sst.shape, dev_sst.shape, test_sst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18083651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  The Rock is destined to be the 21st Century 's...      3\n",
       "1  The gorgeously elaborate continuation of `` Th...      4\n",
       "2  Singer/composer Bryan Adams contributes a slew...      3\n",
       "3  You 'd think by now America would have had eno...      2\n",
       "4               Yet the act is still charming here .      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5761082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.127809\n",
       "1    0.259597\n",
       "2    0.190075\n",
       "3    0.271770\n",
       "4    0.150749\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_sst['label'].value_counts() / train_sst.shape[0]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e567ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8544 entries, 0 to 8543\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  8544 non-null   object\n",
      " 1   label     8544 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 133.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_sst.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea5b07",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f368e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c3fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_sentences = []\n",
    "\n",
    "for sentence in train_sst['sentence'].values:\n",
    "    enc_sent_as_list = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    train_encoded_sentences.append(enc_sent_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b4d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_encoded_sentences = []\n",
    "\n",
    "for sentence in dev_sst['sentence'].values:\n",
    "    enc_sent_as_list = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    dev_encoded_sentences.append(enc_sent_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75678afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array, train_attention_mask_array = create_sentence_input_arrays(\n",
    "    train_encoded_sentences, \n",
    "    MAX_LENGTH\n",
    ")\n",
    "\n",
    "dev_array, dev_attention_mask_array = create_sentence_input_arrays(\n",
    "    dev_encoded_sentences, \n",
    "    MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0b716c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8544, 70), (8544, 70), (1101, 70), (1101, 70))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array.shape, train_attention_mask_array.shape, dev_array.shape, dev_attention_mask_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec894e28",
   "metadata": {},
   "source": [
    "Convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6be285a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(train_array)\n",
    "train_attention_mask_tensor = torch.tensor(train_attention_mask_array)\n",
    "train_labels_tensor = torch.tensor(train_sst['label'].values)\n",
    "\n",
    "dev_tensor = torch.tensor(dev_array)\n",
    "dev_attention_mask_tensor = torch.tensor(dev_attention_mask_array)\n",
    "dev_labels_tensor = torch.tensor(dev_sst['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4ed4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_tensor, train_attention_mask_tensor, train_labels_tensor)\n",
    "dev_dataset = TensorDataset(dev_tensor, dev_attention_mask_tensor, dev_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1096e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=BERT_HYPERPARAMETERS['batch_size'], shuffle=True)\n",
    "dev_data_loader = DataLoader(dev_dataset, batch_size=BERT_HYPERPARAMETERS['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496f790",
   "metadata": {},
   "source": [
    "## Fine-tune BERT\n",
    "\n",
    "Run on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "749bfbd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  1%|          | 3/267 [01:03<1:32:52, 21.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-991f10106ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_tune_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Github/ucl-nlp-group-project/src/models/bert_utils.py\u001b[0m in \u001b[0;36mfine_tune_bert\u001b[0;34m(device, train_data_loader, dev_data_loader)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp-group-project/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp-group-project/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_fine_tuned = fine_tune_bert(device, train_data_loader, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba6445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
