{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bcn-running_config.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlXRAB_l6jcP"
      },
      "source": [
        "# Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOOQ9-ir1CIi"
      },
      "source": [
        "Change the working directory to the one where you saved your files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKDX-Knt6bJL",
        "outputId": "37d82c47-5780-4246-c2a3-5f92e6f2be2d"
      },
      "source": [
        "cd /content/drive/MyDrive/COMP0087/allenNLP/BCN"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/COMP0087/allenNLP/BCN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0N7Dkup24-_"
      },
      "source": [
        "test_set = pd.read_json('/content/drive/MyDrive/COMP0087/data/test.jsonl', orient='records', lines=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVsRQKKHiCsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d5fee6-9a90-46eb-dec3-563b174a3c9b"
      },
      "source": [
        "%%shell\n",
        "pip install torch==1.7.1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.7.4.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jhWemA4za2R"
      },
      "source": [
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import pandas as pd"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1XdJXJ8imFB",
        "outputId": "42bd7c77-90dc-4d3b-d224-b44e114b6b27"
      },
      "source": [
        "%%shell\n",
        "pip install allennlp==2.1.0 allennlp-models==2.1.0"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/bd/c75fa01e3deb9322b637fe0be45164b40d43747661aca9195b5fb334947c/allennlp-2.1.0-py3-none-any.whl (585kB)\n",
            "\u001b[K     |████████████████████████████████| 593kB 5.6MB/s \n",
            "\u001b[?25hCollecting allennlp-models==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/9f/28fff39b61f878939fbccb183ffe4a64ab1043c9f22b62e6a37c35452933/allennlp_models-2.1.0-py3-none-any.whl (407kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 34.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (0.99)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (3.6.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (8.7.0)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/0b/24bb255d19bbe5c645e1bcba6c89ec08b2fcfb4f42341d74b024fee6307d/boto3-1.17.46.tar.gz (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (1.19.5)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 33.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (4.41.1)\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (2.2.4)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (1.4.1)\n",
            "Collecting torchvision<0.9.0,>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 219kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (3.2.5)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (3.0.12)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (2.10.0)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Collecting transformers<4.4,>=4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 16.6MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 33.0MB/s \n",
            "\u001b[?25hCollecting torch<1.8.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 24kB/s \n",
            "\u001b[?25hCollecting py-rouge==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting conllu==4.4\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/be/be6959c3ff2dbfdd87de4be0ccdff577835b5d08b1d25bf7fd4aaf0d7add/conllu-4.4-py2.py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (20.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (54.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (1.4.0)\n",
            "Collecting botocore<1.21.0,>=1.20.46\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/3b/3944ed014ba8180e32054f42944988224b3213c3ee423df0e7a156406c7d/botocore-1.20.46-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 36.9MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.9.0,>=0.8.1->allennlp==2.1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp==2.1.0) (3.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp==2.1.0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 34.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp==2.1.0) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 33.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp==2.1.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.8.0,>=1.6.0->allennlp==2.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp-models==2.1.0) (0.2.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.46->boto3<2.0,>=1.14->allennlp==2.1.0) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==2.1.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.4,>=4.1->allennlp==2.1.0) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.4,>=4.1->allennlp==2.1.0) (7.1.2)\n",
            "Building wheels for collected packages: boto3, jsonnet, overrides, word2number, ftfy, sacremoses\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.17.46-py2.py3-none-any.whl size=128779 sha256=cdd6ac8d58e97e326cc43295763a83ec683795fa2482b16feb13a71f6cbcac91\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/85/22/e21464ad8c886cc3d95c828e4a2a665f7bc01d332ce509e707\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388810 sha256=c0141033d19fb894bdb02b07e03c8470ac68157c029c83183a1b93261955eacd\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=1d54c110cf3a40b2fb41e083f7e3ffd359c1ca640ec9cc65b2c1ecb15dc923a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5589 sha256=2adc50fddef3afe374b5546683fddc925f293062d338f1b55491d66027459191\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=fe993f56af330b8e64f4e3b9c0816b1d4b8e14d52aa5db75279d33b973e21cb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=f327c7d88fad4950ea062ce1c7f663f01204bad9e2fde3e52c9f19bad661d01b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built boto3 jsonnet overrides word2number ftfy sacremoses\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.46 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, jsonnet, sentencepiece, overrides, torch, torchvision, jsonpickle, tokenizers, sacremoses, transformers, tensorboardX, allennlp, py-rouge, word2number, conllu, ftfy, allennlp-models\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed allennlp-2.1.0 allennlp-models-2.1.0 boto3-1.17.46 botocore-1.20.46 conllu-4.4 ftfy-5.9 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 overrides-3.1.0 py-rouge-1.1 s3transfer-0.3.6 sacremoses-0.0.44 sentencepiece-0.1.95 tensorboardX-2.2 tokenizers-0.10.2 torch-1.7.1 torchvision-0.8.2 transformers-4.3.3 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6RPo9iFxHDm"
      },
      "source": [
        "# imports from allennlp\n",
        "from allennlp.models.archival import load_archive\n",
        "from allennlp.common.util import JsonDict\n",
        "from allennlp.data import Instance\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "from allennlp.data.fields import LabelField\n",
        "from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "from overrides import overrides\n",
        "\n",
        "from allennlp.interpret.attackers import Attacker, InputReduction\n",
        "\n",
        "from allennlp.interpret.saliency_interpreters import SimpleGradient"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDNBlc1WxSOS",
        "outputId": "c4fe2824-ee18-4149-a287-c9e5ff06aaa8"
      },
      "source": [
        "%%shell\n",
        "pip install checklist==0.0.10"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting checklist==0.0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/29/854c458733992e53cb1168257ddeae8fea1e70ea3e3b101bc0d32cc81907/checklist-0.0.10.tar.gz (12.1MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1MB 250kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.10) (1.19.5)\n",
            "Requirement already satisfied: spacy>=2.2 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.10) (2.2.4)\n",
            "Collecting munch>=2.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.10) (0.3.3)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.10) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.10) (7.6.3)\n",
            "Requirement already satisfied: transformers>=2.8 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.10) (4.3.3)\n",
            "Collecting patternfork-nosql\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/c9/44df2e48530ff9ebdc0f5a916831aecef2cf10806f3021f09cb4a5040674/patternfork_nosql-3.6.tar.gz (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 2.1MB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/8d/27969852f4e664525c3d070e44b2b719bc195f4d18c311c52e57bb93614e/iso-639-0.4.5.tar.gz (167kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (54.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2->checklist==0.0.10) (2.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch>=2.5->checklist==0.0.10) (1.15.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.10) (5.0.3)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.10) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.10) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.10) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.10) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.10) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.10) (1.0.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.10) (5.0.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.10) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.10) (5.1.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist==0.0.10) (0.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist==0.0.10) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist==0.0.10) (3.8.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist==0.0.10) (0.0.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist==0.0.10) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.8->checklist==0.0.10) (2019.12.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.10) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/26/a6bd68f13e0f38fbb643d6e497fc3462be83a0b6c4d43425c78bb51a7291/backports.csv-1.0.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.10) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.10) (4.2.6)\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.10) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.10) (3.2.5)\n",
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 40.9MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/f9/e11f893dcabe6bc222a1442bf5e14f0322a2d363c92910ed41947078a35a/CherryPy-18.6.0-py2.py3-none-any.whl (419kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2->checklist==0.0.10) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2->checklist==0.0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2->checklist==0.0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2->checklist==0.0.10) (3.0.4)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.10) (22.0.3)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.10) (5.3.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.10) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.10) (2.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.10) (4.7.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.10) (1.9.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter>=1.0->checklist==0.0.10) (1.0.18)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.10) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.10) (3.3.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.10) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.10) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.10) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.10) (1.4.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.10) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.10) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.10) (0.9.3)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.10) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5->checklist==0.0.10) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5->checklist==0.0.10) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5->checklist==0.0.10) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5->checklist==0.0.10) (4.8.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.10) (2.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=2.8->checklist==0.0.10) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=2.8->checklist==0.0.10) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.8->checklist==0.0.10) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.8->checklist==0.0.10) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=2.8->checklist==0.0.10) (2.4.7)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.10) (2.3.0)\n",
            "Collecting zc.lockfile\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/2a/268389776288f0f26c7272c70c36c96dcc0bdb88ab6216ea18e19df1fadd/zc.lockfile-2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.10) (8.7.0)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/fd29409cced540facdd29abb986d988cb1f22c8170d10022ea73af77fa55/portend-2.7.1-py3-none-any.whl\n",
            "Collecting jaraco.collections\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1a/a0d6861d2aca6df92643c755966c8a60e40353e4c5e7a5c2f4e5ed733817/jaraco.collections-3.3.0-py3-none-any.whl\n",
            "Collecting cheroot>=8.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/95/86fe6480af78fea7b0e7e1bf02e6acd4cb9e561ea200bd6d6e1398fe5426/cheroot-8.5.2-py2.py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter>=1.0->checklist==0.0.10) (2.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter>=1.0->checklist==0.0.10) (0.2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.10) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.4->nbconvert->jupyter>=1.0->checklist==0.0.10) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.10) (0.7.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.10) (1.14.5)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading https://files.pythonhosted.org/packages/44/83/4d5c3de53bbc463f30ab6764e27bc2e8ed9b59736e8b40d95403ff802008/tempora-4.0.2-py3-none-any.whl\n",
            "Collecting jaraco.text\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/74/2a3c4835c079df16db8a9c50263eebb0125849fee5b16de353a059b7545d/jaraco.text-3.5.0-py3-none-any.whl\n",
            "Collecting jaraco.classes\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/74/bee5fc11594974746535117546404678fc7b899476e769c3c55bc0cfaa02/jaraco.classes-3.2.1-py3-none-any.whl\n",
            "Collecting jaraco.functools\n",
            "  Downloading https://files.pythonhosted.org/packages/b5/da/e51e7b58c8fe132990edd1e3ef25bcd9801eb7f91d0f642ac7f8d97e4a36/jaraco.functools-3.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.10) (2.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->patternfork-nosql->checklist==0.0.10) (2018.9)\n",
            "Building wheels for collected packages: checklist, patternfork-nosql, iso-639, python-docx, sgmllib3k\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.10-cp37-none-any.whl size=12164434 sha256=69d32ecf0d53ce66c04afb59bd2da784568fe33a73b95786b766133aeaf1dd15\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/a2/02/f075a28620a8370dd79df36bb486095d465e7870623cea6b4e\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-cp37-none-any.whl size=22332807 sha256=1ed0550cfa61dfd097da0fd499ee6f4aa61dc640948583a20202e140e053393e\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/0d/ae/060a851f2104f4cc79380cc57d89f29d77a239597eeecfcf4d\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-cp37-none-any.whl size=169063 sha256=0750bff076cab25c44c98aaf1442cb6f943e09dc93f654c012c5454c39bef86b\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/60/07/73aed7d23ae9b5729970632922ed5e45b535bcd4b8df77ebe9\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp37-none-any.whl size=184491 sha256=e9214452156aded5794b0da5b2832ecbd34110d4d842b6961d17ca75f65171bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=9da2cdd654b70664d557da73760bdc8e3ebf6b94fee8bb7475dbe94b53aaf476\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built checklist patternfork-nosql iso-639 python-docx sgmllib3k\n",
            "Installing collected packages: munch, backports.csv, sgmllib3k, feedparser, cryptography, pdfminer.six, python-docx, zc.lockfile, jaraco.functools, tempora, portend, jaraco.text, jaraco.classes, jaraco.collections, cheroot, cherrypy, patternfork-nosql, iso-639, checklist\n",
            "Successfully installed backports.csv-1.0.7 checklist-0.0.10 cheroot-8.5.2 cherrypy-18.6.0 cryptography-3.4.7 feedparser-6.0.2 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.3.0 jaraco.functools-3.3.0 jaraco.text-3.5.0 munch-2.5.0 patternfork-nosql-3.6 pdfminer.six-20201018 portend-2.7.1 python-docx-0.8.10 sgmllib3k-1.0.0 tempora-4.0.2 zc.lockfile-2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEkzeVRhxQ2L"
      },
      "source": [
        "import checklist\n",
        "from checklist.editor import Editor\n",
        "from checklist.perturb import Perturb"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osq1JxTfV5ID"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02kPpetwu1MS"
      },
      "source": [
        "from scipy.stats import kendalltau\n",
        "from scipy.stats import spearmanr"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YARtVzSoriw3"
      },
      "source": [
        "Background about model: http://docs.allennlp.org/v0.9.0/api/allennlp.models.biattentive_classification_network.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2_NLOzJ9hAM"
      },
      "source": [
        "# importing the dataset reader\n",
        "import tagging\n",
        "# importing the BCN model\n",
        "import BCN_model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3eK_W3n9YDL"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REzuZd1uj0ad",
        "outputId": "ce32115c-fc91-4917-8e42-940d608922e8"
      },
      "source": [
        "# training model\n",
        "# here, the output will be saved to a new folder called 'BCN_output'. You will get an error message if such a directory already exists.\n",
        "!pwd; allennlp train --include-package tagging -s BCN_output config_BCN.jsonnet"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/COMP0087/allenNLP/BCN\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "2021-04-07 10:16:15,656 - ERROR - allennlp.common.plugins - Plugin allennlp_models could not be loaded: No module named 'nltk.translate.meteor_score'\n",
            "2021-04-07 10:16:16,443 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2021-04-07 10:16:16,449 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2021-04-07 10:16:16,450 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2021-04-07 10:16:16,450 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2021-04-07 10:16:16,519 - INFO - allennlp.common.checks - Pytorch version: 1.7.1\n",
            "2021-04-07 10:16:16,520 - INFO - allennlp.common.params - type = default\n",
            "2021-04-07 10:16:16,521 - INFO - allennlp.common.params - dataset_reader.type = ag_reader\n",
            "2021-04-07 10:16:16,522 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2021-04-07 10:16:16,522 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2021-04-07 10:16:16,522 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-04-07 10:16:16,522 - INFO - allennlp.common.params - dataset_reader.token_indexers = None\n",
            "2021-04-07 10:16:16,523 - INFO - allennlp.common.params - dataset_reader.tokenizer = None\n",
            "2021-04-07 10:16:16,523 - INFO - allennlp.common.params - dataset_reader.segment_sentences = False\n",
            "2021-04-07 10:16:16,523 - INFO - allennlp.common.params - dataset_reader.max_sequence_length = None\n",
            "2021-04-07 10:16:16,523 - INFO - allennlp.common.params - dataset_reader.skip_label_indexing = False\n",
            "2021-04-07 10:16:17,102 - INFO - allennlp.common.params - train_data_path = /content/drive/MyDrive/COMP0087/data/train.json\n",
            "2021-04-07 10:16:17,102 - INFO - allennlp.common.params - vocabulary = <allennlp.common.lazy.Lazy object at 0x7f8f65fc4d50>\n",
            "2021-04-07 10:16:17,103 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2021-04-07 10:16:17,103 - INFO - allennlp.common.params - validation_dataset_reader.type = ag_reader\n",
            "2021-04-07 10:16:17,104 - INFO - allennlp.common.params - validation_dataset_reader.max_instances = None\n",
            "2021-04-07 10:16:17,104 - INFO - allennlp.common.params - validation_dataset_reader.manual_distributed_sharding = False\n",
            "2021-04-07 10:16:17,104 - INFO - allennlp.common.params - validation_dataset_reader.manual_multiprocess_sharding = False\n",
            "2021-04-07 10:16:17,105 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers = None\n",
            "2021-04-07 10:16:17,105 - INFO - allennlp.common.params - validation_dataset_reader.tokenizer = None\n",
            "2021-04-07 10:16:17,105 - INFO - allennlp.common.params - validation_dataset_reader.segment_sentences = False\n",
            "2021-04-07 10:16:17,105 - INFO - allennlp.common.params - validation_dataset_reader.max_sequence_length = None\n",
            "2021-04-07 10:16:17,106 - INFO - allennlp.common.params - validation_dataset_reader.skip_label_indexing = False\n",
            "2021-04-07 10:16:17,106 - INFO - allennlp.common.params - validation_data_path = /content/drive/MyDrive/COMP0087/data/val.json\n",
            "2021-04-07 10:16:17,106 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2021-04-07 10:16:17,107 - INFO - allennlp.common.params - test_data_path = None\n",
            "2021-04-07 10:16:17,107 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2021-04-07 10:16:17,107 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2021-04-07 10:16:17,108 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-04-07 10:16:17,108 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-04-07 10:16:17,109 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-04-07 10:16:17,109 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-04-07 10:16:17,110 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-04-07 10:16:17,110 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 100\n",
            "2021-04-07 10:16:17,110 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None\n",
            "2021-04-07 10:16:17,111 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-04-07 10:16:17,111 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-04-07 10:16:17,111 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-04-07 10:16:17,111 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-04-07 10:16:17,112 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-04-07 10:16:17,112 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-04-07 10:16:17,112 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "loading instances: 108000it [01:03, 1692.30it/s]\n",
            "2021-04-07 10:17:20,932 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2021-04-07 10:17:20,932 - INFO - allennlp.common.params - data_loader.batch_size = None\n",
            "2021-04-07 10:17:20,933 - INFO - allennlp.common.params - data_loader.drop_last = False\n",
            "2021-04-07 10:17:20,933 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2021-04-07 10:17:20,933 - INFO - allennlp.common.params - data_loader.batch_sampler.type = bucket\n",
            "2021-04-07 10:17:20,934 - INFO - allennlp.common.params - data_loader.batch_sampler.batch_size = 100\n",
            "2021-04-07 10:17:20,934 - INFO - allennlp.common.params - data_loader.batch_sampler.sorting_keys = None\n",
            "2021-04-07 10:17:20,934 - INFO - allennlp.common.params - data_loader.batch_sampler.padding_noise = 0.1\n",
            "2021-04-07 10:17:20,935 - INFO - allennlp.common.params - data_loader.batch_sampler.drop_last = False\n",
            "2021-04-07 10:17:20,935 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2021-04-07 10:17:20,935 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2021-04-07 10:17:20,935 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2021-04-07 10:17:20,936 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2021-04-07 10:17:20,936 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "loading instances: 12000it [00:07, 1565.09it/s]\n",
            "2021-04-07 10:17:28,604 - INFO - allennlp.common.params - type = from_instances\n",
            "2021-04-07 10:17:28,605 - INFO - allennlp.common.params - min_count = None\n",
            "2021-04-07 10:17:28,605 - INFO - allennlp.common.params - max_vocab_size = None\n",
            "2021-04-07 10:17:28,605 - INFO - allennlp.common.params - non_padded_namespaces = ('*tags', '*labels')\n",
            "2021-04-07 10:17:28,606 - INFO - allennlp.common.params - pretrained_files = None\n",
            "2021-04-07 10:17:28,606 - INFO - allennlp.common.params - only_include_pretrained_words = False\n",
            "2021-04-07 10:17:28,606 - INFO - allennlp.common.params - tokens_to_add = None\n",
            "2021-04-07 10:17:28,607 - INFO - allennlp.common.params - min_pretrained_embeddings = None\n",
            "2021-04-07 10:17:28,607 - INFO - allennlp.common.params - padding_token = @@PADDING@@\n",
            "2021-04-07 10:17:28,607 - INFO - allennlp.common.params - oov_token = @@UNKNOWN@@\n",
            "2021-04-07 10:17:28,607 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\n",
            "building vocab: 120000it [00:03, 30976.93it/s]\n",
            "2021-04-07 10:17:32,669 - INFO - allennlp.common.params - model.type = bcn\n",
            "2021-04-07 10:17:32,670 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2021-04-07 10:17:32,670 - INFO - allennlp.common.params - model.text_field_embedder.type = basic\n",
            "2021-04-07 10:17:32,671 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding\n",
            "2021-04-07 10:17:32,671 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 300\n",
            "2021-04-07 10:17:32,671 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n",
            "2021-04-07 10:17:32,672 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
            "2021-04-07 10:17:32,672 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.weight = None\n",
            "2021-04-07 10:17:32,672 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None\n",
            "2021-04-07 10:17:32,673 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = False\n",
            "2021-04-07 10:17:32,673 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
            "2021-04-07 10:17:32,673 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
            "2021-04-07 10:17:32,673 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
            "2021-04-07 10:17:32,674 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False\n",
            "2021-04-07 10:17:32,674 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
            "2021-04-07 10:17:32,674 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = https://allennlp.s3.amazonaws.com/datasets/glove/glove.840B.300d.txt.gz\n",
            "2021-04-07 10:17:32,691 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\n",
            "2021-04-07 10:17:33,273 - INFO - allennlp.common.file_utils - https://allennlp.s3.amazonaws.com/datasets/glove/glove.840B.300d.txt.gz not found in cache, downloading to /root/.allennlp/cache/a21f3138db1e7a0f6259a4a0b086099b50c73101bbe4182ba8688f41348388a3.8de7c14eda15038006ebf553b708bcae9c046b2ecf71dbbbedd8b0b9d0d0a621\n",
            "downloading: 100%|##########| 2176768669/2176768669 [01:50<00:00, 19636673.96B/s]\n",
            "2196017it [00:58, 37588.01it/s]\n",
            "2021-04-07 10:20:23,458 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\n",
            "2021-04-07 10:20:24,868 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 70316 out of 95889 tokens\n",
            "2021-04-07 10:20:24,912 - INFO - allennlp.common.params - model.embedding_dropout = 0.25\n",
            "2021-04-07 10:20:24,912 - INFO - allennlp.common.params - model.pre_encode_feedforward.input_dim = 300\n",
            "2021-04-07 10:20:24,913 - INFO - allennlp.common.params - model.pre_encode_feedforward.num_layers = 1\n",
            "2021-04-07 10:20:24,913 - INFO - allennlp.common.params - model.pre_encode_feedforward.hidden_dims = [300]\n",
            "2021-04-07 10:20:24,914 - INFO - allennlp.common.params - model.pre_encode_feedforward.activations = ['relu']\n",
            "2021-04-07 10:20:24,916 - INFO - allennlp.common.params - type = relu\n",
            "2021-04-07 10:20:24,916 - INFO - allennlp.common.params - model.pre_encode_feedforward.dropout = [0.25]\n",
            "2021-04-07 10:20:24,918 - INFO - allennlp.common.params - model.encoder.type = lstm\n",
            "2021-04-07 10:20:24,918 - INFO - allennlp.common.params - model.encoder.input_size = 300\n",
            "2021-04-07 10:20:24,919 - INFO - allennlp.common.params - model.encoder.hidden_size = 300\n",
            "2021-04-07 10:20:24,919 - INFO - allennlp.common.params - model.encoder.num_layers = 1\n",
            "2021-04-07 10:20:24,919 - INFO - allennlp.common.params - model.encoder.bias = True\n",
            "2021-04-07 10:20:24,920 - INFO - allennlp.common.params - model.encoder.dropout = 0.0\n",
            "2021-04-07 10:20:24,920 - INFO - allennlp.common.params - model.encoder.bidirectional = True\n",
            "2021-04-07 10:20:24,920 - INFO - allennlp.common.params - model.encoder.stateful = False\n",
            "2021-04-07 10:20:24,932 - INFO - allennlp.common.params - model.integrator.type = lstm\n",
            "2021-04-07 10:20:24,932 - INFO - allennlp.common.params - model.integrator.input_size = 1800\n",
            "2021-04-07 10:20:24,932 - INFO - allennlp.common.params - model.integrator.hidden_size = 300\n",
            "2021-04-07 10:20:24,933 - INFO - allennlp.common.params - model.integrator.num_layers = 1\n",
            "2021-04-07 10:20:24,933 - INFO - allennlp.common.params - model.integrator.bias = True\n",
            "2021-04-07 10:20:24,933 - INFO - allennlp.common.params - model.integrator.dropout = 0.0\n",
            "2021-04-07 10:20:24,933 - INFO - allennlp.common.params - model.integrator.bidirectional = True\n",
            "2021-04-07 10:20:24,934 - INFO - allennlp.common.params - model.integrator.stateful = False\n",
            "2021-04-07 10:20:24,972 - INFO - allennlp.common.params - model.integrator_dropout = 0.1\n",
            "2021-04-07 10:20:24,972 - INFO - allennlp.common.params - model.output_layer.input_dim = 2400\n",
            "2021-04-07 10:20:24,973 - INFO - allennlp.common.params - model.output_layer.num_layers = 3\n",
            "2021-04-07 10:20:24,973 - INFO - allennlp.common.params - model.output_layer.input_dim = 2400\n",
            "2021-04-07 10:20:24,973 - INFO - allennlp.common.params - model.output_layer.num_layers = 3\n",
            "2021-04-07 10:20:24,974 - INFO - allennlp.common.params - model.output_layer.output_dims = [1200, 600, 4]\n",
            "2021-04-07 10:20:24,974 - INFO - allennlp.common.params - model.output_layer.pool_sizes = 4\n",
            "2021-04-07 10:20:24,975 - INFO - allennlp.common.params - model.output_layer.dropout = [0.2, 0.3, 0]\n",
            "2021-04-07 10:20:25,096 - INFO - allennlp.common.params - model.elmo = None\n",
            "2021-04-07 10:20:25,096 - INFO - allennlp.common.params - model.use_input_elmo = False\n",
            "2021-04-07 10:20:25,096 - INFO - allennlp.common.params - model.use_integrator_output_elmo = False\n",
            "2021-04-07 10:20:25,097 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f903ae28390>\n",
            "2021-04-07 10:20:25,097 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2021-04-07 10:20:25,098 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2021-04-07 10:20:25,098 - INFO - allennlp.nn.initializers -    _encoder._module.bias_hh_l0\n",
            "2021-04-07 10:20:25,099 - INFO - allennlp.nn.initializers -    _encoder._module.bias_hh_l0_reverse\n",
            "2021-04-07 10:20:25,099 - INFO - allennlp.nn.initializers -    _encoder._module.bias_ih_l0\n",
            "2021-04-07 10:20:25,099 - INFO - allennlp.nn.initializers -    _encoder._module.bias_ih_l0_reverse\n",
            "2021-04-07 10:20:25,099 - INFO - allennlp.nn.initializers -    _encoder._module.weight_hh_l0\n",
            "2021-04-07 10:20:25,100 - INFO - allennlp.nn.initializers -    _encoder._module.weight_hh_l0_reverse\n",
            "2021-04-07 10:20:25,100 - INFO - allennlp.nn.initializers -    _encoder._module.weight_ih_l0\n",
            "2021-04-07 10:20:25,100 - INFO - allennlp.nn.initializers -    _encoder._module.weight_ih_l0_reverse\n",
            "2021-04-07 10:20:25,100 - INFO - allennlp.nn.initializers -    _integrator._module.bias_hh_l0\n",
            "2021-04-07 10:20:25,101 - INFO - allennlp.nn.initializers -    _integrator._module.bias_hh_l0_reverse\n",
            "2021-04-07 10:20:25,101 - INFO - allennlp.nn.initializers -    _integrator._module.bias_ih_l0\n",
            "2021-04-07 10:20:25,101 - INFO - allennlp.nn.initializers -    _integrator._module.bias_ih_l0_reverse\n",
            "2021-04-07 10:20:25,101 - INFO - allennlp.nn.initializers -    _integrator._module.weight_hh_l0\n",
            "2021-04-07 10:20:25,108 - INFO - allennlp.nn.initializers -    _integrator._module.weight_hh_l0_reverse\n",
            "2021-04-07 10:20:25,109 - INFO - allennlp.nn.initializers -    _integrator._module.weight_ih_l0\n",
            "2021-04-07 10:20:25,109 - INFO - allennlp.nn.initializers -    _integrator._module.weight_ih_l0_reverse\n",
            "2021-04-07 10:20:25,109 - INFO - allennlp.nn.initializers -    _output_layer._linear_layers.0.bias\n",
            "2021-04-07 10:20:25,109 - INFO - allennlp.nn.initializers -    _output_layer._linear_layers.0.weight\n",
            "2021-04-07 10:20:25,110 - INFO - allennlp.nn.initializers -    _output_layer._linear_layers.1.bias\n",
            "2021-04-07 10:20:25,110 - INFO - allennlp.nn.initializers -    _output_layer._linear_layers.1.weight\n",
            "2021-04-07 10:20:25,110 - INFO - allennlp.nn.initializers -    _output_layer._linear_layers.2.bias\n",
            "2021-04-07 10:20:25,111 - INFO - allennlp.nn.initializers -    _output_layer._linear_layers.2.weight\n",
            "2021-04-07 10:20:25,111 - INFO - allennlp.nn.initializers -    _pre_encode_feedforward._linear_layers.0.bias\n",
            "2021-04-07 10:20:25,111 - INFO - allennlp.nn.initializers -    _pre_encode_feedforward._linear_layers.0.weight\n",
            "2021-04-07 10:20:25,111 - INFO - allennlp.nn.initializers -    _self_attentive_pooling_projection.bias\n",
            "2021-04-07 10:20:25,111 - INFO - allennlp.nn.initializers -    _self_attentive_pooling_projection.weight\n",
            "2021-04-07 10:20:25,112 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight\n",
            "2021-04-07 10:20:31,274 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2021-04-07 10:20:31,275 - INFO - allennlp.common.params - trainer.patience = 5\n",
            "2021-04-07 10:20:31,275 - INFO - allennlp.common.params - trainer.validation_metric = +accuracy\n",
            "2021-04-07 10:20:31,276 - INFO - allennlp.common.params - trainer.num_epochs = 5\n",
            "2021-04-07 10:20:31,276 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2021-04-07 10:20:31,277 - INFO - allennlp.common.params - trainer.grad_norm = 5\n",
            "2021-04-07 10:20:31,277 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2021-04-07 10:20:31,277 - INFO - allennlp.common.params - trainer.distributed = False\n",
            "2021-04-07 10:20:31,278 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2021-04-07 10:20:31,278 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2021-04-07 10:20:31,278 - INFO - allennlp.common.params - trainer.use_amp = False\n",
            "2021-04-07 10:20:31,278 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2021-04-07 10:20:31,279 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None\n",
            "2021-04-07 10:20:31,279 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2021-04-07 10:20:31,279 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2021-04-07 10:20:31,280 - INFO - allennlp.common.params - trainer.checkpointer = <allennlp.common.lazy.Lazy object at 0x7f8f6601aad0>\n",
            "2021-04-07 10:20:31,280 - INFO - allennlp.common.params - trainer.callbacks = None\n",
            "2021-04-07 10:20:31,280 - INFO - allennlp.common.params - trainer.trainer_callbacks = None\n",
            "2021-04-07 10:20:36,905 - INFO - allennlp.common.params - trainer.optimizer.type = adam\n",
            "2021-04-07 10:20:36,906 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\n",
            "2021-04-07 10:20:36,906 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.001\n",
            "2021-04-07 10:20:36,906 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2021-04-07 10:20:36,907 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-08\n",
            "2021-04-07 10:20:36,907 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.0\n",
            "2021-04-07 10:20:36,907 - INFO - allennlp.common.params - trainer.optimizer.amsgrad = False\n",
            "2021-04-07 10:20:36,908 - INFO - allennlp.training.optimizers - Number of trainable parameters: 20997317\n",
            "2021-04-07 10:20:36,908 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2021-04-07 10:20:36,909 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.weight\n",
            "2021-04-07 10:20:36,909 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2021-04-07 10:20:36,909 - INFO - allennlp.common.util - _pre_encode_feedforward._linear_layers.0.weight\n",
            "2021-04-07 10:20:36,910 - INFO - allennlp.common.util - _pre_encode_feedforward._linear_layers.0.bias\n",
            "2021-04-07 10:20:36,910 - INFO - allennlp.common.util - _encoder._module.weight_ih_l0\n",
            "2021-04-07 10:20:36,910 - INFO - allennlp.common.util - _encoder._module.weight_hh_l0\n",
            "2021-04-07 10:20:36,910 - INFO - allennlp.common.util - _encoder._module.bias_ih_l0\n",
            "2021-04-07 10:20:36,911 - INFO - allennlp.common.util - _encoder._module.bias_hh_l0\n",
            "2021-04-07 10:20:36,911 - INFO - allennlp.common.util - _encoder._module.weight_ih_l0_reverse\n",
            "2021-04-07 10:20:36,911 - INFO - allennlp.common.util - _encoder._module.weight_hh_l0_reverse\n",
            "2021-04-07 10:20:36,911 - INFO - allennlp.common.util - _encoder._module.bias_ih_l0_reverse\n",
            "2021-04-07 10:20:36,912 - INFO - allennlp.common.util - _encoder._module.bias_hh_l0_reverse\n",
            "2021-04-07 10:20:36,912 - INFO - allennlp.common.util - _integrator._module.weight_ih_l0\n",
            "2021-04-07 10:20:36,912 - INFO - allennlp.common.util - _integrator._module.weight_hh_l0\n",
            "2021-04-07 10:20:36,913 - INFO - allennlp.common.util - _integrator._module.bias_ih_l0\n",
            "2021-04-07 10:20:36,913 - INFO - allennlp.common.util - _integrator._module.bias_hh_l0\n",
            "2021-04-07 10:20:36,913 - INFO - allennlp.common.util - _integrator._module.weight_ih_l0_reverse\n",
            "2021-04-07 10:20:36,913 - INFO - allennlp.common.util - _integrator._module.weight_hh_l0_reverse\n",
            "2021-04-07 10:20:36,914 - INFO - allennlp.common.util - _integrator._module.bias_ih_l0_reverse\n",
            "2021-04-07 10:20:36,914 - INFO - allennlp.common.util - _integrator._module.bias_hh_l0_reverse\n",
            "2021-04-07 10:20:36,914 - INFO - allennlp.common.util - _self_attentive_pooling_projection.weight\n",
            "2021-04-07 10:20:36,914 - INFO - allennlp.common.util - _self_attentive_pooling_projection.bias\n",
            "2021-04-07 10:20:36,915 - INFO - allennlp.common.util - _output_layer._linear_layers.0.weight\n",
            "2021-04-07 10:20:36,915 - INFO - allennlp.common.util - _output_layer._linear_layers.0.bias\n",
            "2021-04-07 10:20:36,915 - INFO - allennlp.common.util - _output_layer._linear_layers.1.weight\n",
            "2021-04-07 10:20:36,916 - INFO - allennlp.common.util - _output_layer._linear_layers.1.bias\n",
            "2021-04-07 10:20:36,916 - INFO - allennlp.common.util - _output_layer._linear_layers.2.weight\n",
            "2021-04-07 10:20:36,916 - INFO - allennlp.common.util - _output_layer._linear_layers.2.bias\n",
            "2021-04-07 10:20:36,916 - INFO - allennlp.common.params - type = default\n",
            "2021-04-07 10:20:36,917 - INFO - allennlp.common.params - keep_serialized_model_every_num_seconds = None\n",
            "2021-04-07 10:20:36,917 - INFO - allennlp.common.params - num_serialized_models_to_keep = 2\n",
            "2021-04-07 10:20:36,918 - INFO - allennlp.common.params - model_save_interval = None\n",
            "2021-04-07 10:20:36,919 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2021-04-07 10:20:36,919 - INFO - allennlp.training.trainer - Epoch 0/4\n",
            "2021-04-07 10:20:36,920 - INFO - allennlp.training.trainer - Worker 0 memory usage: 3.1G\n",
            "2021-04-07 10:20:36,920 - INFO - allennlp.training.trainer - GPU 0 memory usage: 191M\n",
            "2021-04-07 10:20:36,921 - INFO - allennlp.training.trainer - Training\n",
            "  0%|          | 0/1080 [00:00<?, ?it/s]2021-04-07 10:20:37,001 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one\n",
            "2021-04-07 10:20:37,001 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys\n",
            "accuracy: 0.8739, accuracy3: 0.9918, batch_loss: 0.2345, loss: 0.3595 ||: 100%|##########| 1080/1080 [02:55<00:00,  6.15it/s]\n",
            "2021-04-07 10:23:33,497 - INFO - allennlp.training.trainer - Validating\n",
            "  0%|          | 0/120 [00:00<?, ?it/s]2021-04-07 10:23:33,509 - INFO - allennlp.data.samplers.bucket_batch_sampler - No sorting keys given; trying to guess a good one\n",
            "2021-04-07 10:23:33,509 - INFO - allennlp.data.samplers.bucket_batch_sampler - Using ['tokens'] as the sorting keys\n",
            "accuracy: 0.9065, accuracy3: 0.9965, batch_loss: 0.3838, loss: 0.2561 ||: 100%|##########| 120/120 [00:07<00:00, 16.76it/s]\n",
            "2021-04-07 10:23:41,576 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'BCN_output/best.th'.\n",
            "2021-04-07 10:23:42,968 - INFO - allennlp.training.trainer - Epoch duration: 0:03:06.048630\n",
            "2021-04-07 10:23:42,969 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:12:24\n",
            "2021-04-07 10:23:42,969 - INFO - allennlp.training.trainer - Epoch 1/4\n",
            "2021-04-07 10:23:42,970 - INFO - allennlp.training.trainer - Worker 0 memory usage: 3.4G\n",
            "2021-04-07 10:23:42,970 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.9G\n",
            "2021-04-07 10:23:42,971 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9068, accuracy3: 0.9967, batch_loss: 0.1973, loss: 0.2708 ||: 100%|##########| 1080/1080 [02:55<00:00,  6.14it/s]\n",
            "2021-04-07 10:26:39,879 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9154, accuracy3: 0.9978, batch_loss: 0.1350, loss: 0.2360 ||: 100%|##########| 120/120 [00:07<00:00, 16.90it/s]\n",
            "2021-04-07 10:26:47,869 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'BCN_output/best.th'.\n",
            "2021-04-07 10:26:49,526 - INFO - allennlp.training.trainer - Epoch duration: 0:03:06.556378\n",
            "2021-04-07 10:26:49,534 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:09:18\n",
            "2021-04-07 10:26:49,536 - INFO - allennlp.training.trainer - Epoch 2/4\n",
            "2021-04-07 10:26:49,536 - INFO - allennlp.training.trainer - Worker 0 memory usage: 3.4G\n",
            "2021-04-07 10:26:49,537 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.9G\n",
            "2021-04-07 10:26:49,537 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9145, accuracy3: 0.9972, batch_loss: 0.1841, loss: 0.2461 ||: 100%|##########| 1080/1080 [02:58<00:00,  6.05it/s]\n",
            "2021-04-07 10:29:49,079 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9222, accuracy3: 0.9980, batch_loss: 0.1840, loss: 0.2248 ||: 100%|##########| 120/120 [00:07<00:00, 16.91it/s]\n",
            "2021-04-07 10:29:57,090 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'BCN_output/best.th'.\n",
            "2021-04-07 10:29:58,624 - INFO - allennlp.training.trainer - Epoch duration: 0:03:09.087906\n",
            "2021-04-07 10:29:58,624 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:06:14\n",
            "2021-04-07 10:29:58,626 - INFO - allennlp.training.trainer - Epoch 3/4\n",
            "2021-04-07 10:29:58,626 - INFO - allennlp.training.trainer - Worker 0 memory usage: 3.4G\n",
            "2021-04-07 10:29:58,627 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.9G\n",
            "2021-04-07 10:29:58,627 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9210, accuracy3: 0.9979, batch_loss: 0.4098, loss: 0.2264 ||: 100%|##########| 1080/1080 [02:56<00:00,  6.13it/s]\n",
            "2021-04-07 10:32:55,663 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9241, accuracy3: 0.9982, batch_loss: 0.1976, loss: 0.2177 ||: 100%|##########| 120/120 [00:07<00:00, 16.94it/s]\n",
            "2021-04-07 10:33:03,502 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'BCN_output/best.th'.\n",
            "2021-04-07 10:33:05,032 - INFO - allennlp.training.trainer - Epoch duration: 0:03:06.405846\n",
            "2021-04-07 10:33:05,032 - INFO - allennlp.training.trainer - Estimated training time remaining: 0:03:07\n",
            "2021-04-07 10:33:05,033 - INFO - allennlp.training.trainer - Epoch 4/4\n",
            "2021-04-07 10:33:05,033 - INFO - allennlp.training.trainer - Worker 0 memory usage: 3.4G\n",
            "2021-04-07 10:33:05,034 - INFO - allennlp.training.trainer - GPU 0 memory usage: 1.9G\n",
            "2021-04-07 10:33:05,034 - INFO - allennlp.training.trainer - Training\n",
            "accuracy: 0.9248, accuracy3: 0.9978, batch_loss: 0.1941, loss: 0.2145 ||: 100%|##########| 1080/1080 [02:56<00:00,  6.13it/s]\n",
            "2021-04-07 10:36:02,098 - INFO - allennlp.training.trainer - Validating\n",
            "accuracy: 0.9277, accuracy3: 0.9978, batch_loss: 0.2437, loss: 0.2108 ||: 100%|##########| 120/120 [00:07<00:00, 17.00it/s]\n",
            "2021-04-07 10:36:09,951 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'BCN_output/best.th'.\n",
            "2021-04-07 10:36:11,727 - INFO - allennlp.training.trainer - Epoch duration: 0:03:06.694646\n",
            "2021-04-07 10:36:11,728 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2021-04-07 10:36:12,144 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 4,\n",
            "  \"peak_worker_0_memory_MB\": 3484.33984375,\n",
            "  \"peak_gpu_0_memory_MB\": 1903.341796875,\n",
            "  \"training_duration\": \"0:15:32.239702\",\n",
            "  \"training_start_epoch\": 0,\n",
            "  \"training_epochs\": 4,\n",
            "  \"epoch\": 4,\n",
            "  \"training_accuracy\": 0.9247777777777778,\n",
            "  \"training_accuracy3\": 0.9977870370370371,\n",
            "  \"training_loss\": 0.21454569346581895,\n",
            "  \"training_worker_0_memory_MB\": 3484.33984375,\n",
            "  \"training_gpu_0_memory_MB\": 1903.341796875,\n",
            "  \"validation_accuracy\": 0.92775,\n",
            "  \"validation_accuracy3\": 0.9978333333333333,\n",
            "  \"validation_loss\": 0.21077454732730985,\n",
            "  \"best_validation_accuracy\": 0.92775,\n",
            "  \"best_validation_accuracy3\": 0.9978333333333333,\n",
            "  \"best_validation_loss\": 0.21077454732730985\n",
            "}\n",
            "2021-04-07 10:36:13,175 - INFO - allennlp.models.archival - archiving weights and vocabulary to BCN_output/model.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KAf30nQ-HGa"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtuiSt0r88NH"
      },
      "source": [
        "@Predictor.register('ag_text_classifier')\n",
        "class AGNewsClassifier(Predictor):\n",
        "    \"\"\"\n",
        "    Predictor for any model that takes in a sentence and returns\n",
        "    a single class for it.  In particular, it can be used with\n",
        "    the [`BasicClassifier`](../models/basic_classifier.md) model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def predict(self, sentence: str) -> JsonDict:\n",
        "        return self.predict_json({\"Description\": sentence})\n",
        "\n",
        "    @overrides\n",
        "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
        "        \"\"\"\n",
        "        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n",
        "        Runs the underlying model, and adds the `\"label\"` to the output.\n",
        "        \"\"\"\n",
        "        sentence = json_dict[\"Description\"]\n",
        "        reader_has_tokenizer = (\n",
        "            getattr(self._dataset_reader, \"tokenizer\", None) is not None\n",
        "            or getattr(self._dataset_reader, \"_tokenizer\", None) is not None\n",
        "        )\n",
        "        if not reader_has_tokenizer:\n",
        "            tokenizer = SpacyTokenizer()\n",
        "            sentence = tokenizer.tokenize(sentence)\n",
        "        return self._dataset_reader.text_to_instance(sentence)\n",
        "\n",
        "    @overrides\n",
        "    def predictions_to_labeled_instances(\n",
        "        self, instance: Instance, outputs: Dict[str, np.ndarray]\n",
        "    ) -> List[Instance]:\n",
        "        new_instance = instance.duplicate()\n",
        "        label = np.argmax(outputs[\"class_probabilities\"])\n",
        "        new_instance.add_field(\"label\", LabelField(int(label), skip_indexing=True))\n",
        "        return [new_instance]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFbFxippbuWV"
      },
      "source": [
        "archive = load_archive(\"./BCN_output/model.tar.gz\")\n",
        "model = archive.model\n",
        "vocab = model.vocab"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I2Y8xiVsH6W"
      },
      "source": [
        "predictor = Predictor.from_archive(archive, 'ag_text_classifier')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIq7HL_F0n7X",
        "outputId": "6ce9c4db-2f67-4527-f879-f6d3d6e73fe7"
      },
      "source": [
        "predictor.predict(sentence=\"Canadian Press - VANCOUVER (CP) - The sister of a man who died after a violent confrontation with police has demanded the city's chief constable resign for defending the officer involved.\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_probabilities': [2.405789976955841e-11,\n",
              "  4.568966868257265e-15,\n",
              "  1.0,\n",
              "  3.932658886041107e-12],\n",
              " 'label': '1',\n",
              " 'logits': [2.251049280166626,\n",
              "  -6.3178839683532715,\n",
              "  26.70160675048828,\n",
              "  0.43990111351013184]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9_kbyfmCKgB"
      },
      "source": [
        "# Interpret"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNcaEtePRl4G"
      },
      "source": [
        "@Predictor.register('ag_text_classifier_with_input_red')\n",
        "class InputReductionTextClassifierPredictor(AGNewsClassifier):\n",
        "    \n",
        "    def predict_json(self, json_dict: JsonDict) -> JsonDict:\n",
        "        predictor = AGNewsClassifier(self._model, self._dataset_reader)\n",
        "        prediction = predictor.predict(sentence=json_dict['Description'])\n",
        "\n",
        "        attacker = InputReduction(predictor)\n",
        "        attack = attacker.attack_from_json(inputs=json_dict,\n",
        "                                           input_field_to_attack='tokens',\n",
        "                                           grad_input_field='grad_input_1',\n",
        "                                           ignore_tokens=None)\n",
        "\n",
        "        return {'prediction': prediction, 'input_reduction_output': attack}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBPyBfF4tE1P"
      },
      "source": [
        "predictor_with_input_red = Predictor.from_archive(archive, 'ag_text_classifier_with_input_red')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ7Fe8miYAIK"
      },
      "source": [
        "input = \"The Cleveland Indians pulled within one game of the AL Central lead by beating the Minnesota Twins, 7-1, Saturday night with home runs by Travis Hafner and Victor Martinez.\""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "144t41eqpO1T",
        "outputId": "9ba56d53-0503-4586-ac26-5dc6ecbbeb3d"
      },
      "source": [
        "predictor_with_input_red.predict(input)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_reduction_output': {'final': [['pulled', 'Hafner']],\n",
              "  'original': ['The',\n",
              "   'Cleveland',\n",
              "   'Indians',\n",
              "   'pulled',\n",
              "   'within',\n",
              "   'one',\n",
              "   'game',\n",
              "   'of',\n",
              "   'the',\n",
              "   'AL',\n",
              "   'Central',\n",
              "   'lead',\n",
              "   'by',\n",
              "   'beating',\n",
              "   'the',\n",
              "   'Minnesota',\n",
              "   'Twins',\n",
              "   ',',\n",
              "   '7',\n",
              "   '-',\n",
              "   '1',\n",
              "   ',',\n",
              "   'Saturday',\n",
              "   'night',\n",
              "   'with',\n",
              "   'home',\n",
              "   'runs',\n",
              "   'by',\n",
              "   'Travis',\n",
              "   'Hafner',\n",
              "   'and',\n",
              "   'Victor',\n",
              "   'Martinez',\n",
              "   '.']},\n",
              " 'prediction': {'class_probabilities': [0.000256297062151134,\n",
              "   0.9993062019348145,\n",
              "   0.0002524169394746423,\n",
              "   0.00018510186055209488],\n",
              "  'label': '2',\n",
              "  'logits': [-1.1653821468353271,\n",
              "   7.103096961975098,\n",
              "   -1.1806373596191406,\n",
              "   -1.4908137321472168]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wLMMnMStP1S"
      },
      "source": [
        "result = predictor_with_input_red.predict(input)\n",
        "tokens = result['input_reduction_output']['original']\n",
        "\n",
        "grad = SimpleGradient(predictor).saliency_interpret_from_json({'Description':input})\n",
        "gradient_list = grad['instance_1']['grad_input_1']"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC6gSe9xRWia"
      },
      "source": [
        "# Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc26204rRtVU"
      },
      "source": [
        "Source: https://adataanalyst.com/machine-learning/highlight-text-using-weights/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtonm9haRVkw"
      },
      "source": [
        "import html\n",
        "import random\n",
        "from IPython.core.display import display, HTML"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXExZzDKRpNW"
      },
      "source": [
        "# Prevent special characters like & and < to cause the browser to display something other than what you intended.\n",
        "def html_escape(text):\n",
        "    return html.escape(text)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ghrxcdS5EN"
      },
      "source": [
        "max_alpha = 0.4 \n",
        "highlighted_text = []\n",
        "for i in range(len(tokens)):\n",
        "    weight = gradient_list[i]\n",
        "    highlighted_text.append('<span style=\"background-color:rgba(135,206,250,' + str(weight / max_alpha) + ');\">' + html_escape(tokens[i]) + '</span>')\n",
        "highlighted_text = ' '.join(highlighted_text)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "j4E_1_N_TMDa",
        "outputId": "2447d304-c234-4707-c1e2-c0bd5d71c6a4"
      },
      "source": [
        "print(display(HTML(highlighted_text)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,0.13552817252980862);\">The</span> <span style=\"background-color:rgba(135,206,250,0.15684464000889772);\">Cleveland</span> <span style=\"background-color:rgba(135,206,250,0.07846217963787293);\">Indians</span> <span style=\"background-color:rgba(135,206,250,0.20684975872519948);\">pulled</span> <span style=\"background-color:rgba(135,206,250,0.13018946201659726);\">within</span> <span style=\"background-color:rgba(135,206,250,0.0055155787498704515);\">one</span> <span style=\"background-color:rgba(135,206,250,0.11103269785418357);\">game</span> <span style=\"background-color:rgba(135,206,250,0.040989732234262784);\">of</span> <span style=\"background-color:rgba(135,206,250,0.008429994524237436);\">the</span> <span style=\"background-color:rgba(135,206,250,0.14649536977353786);\">AL</span> <span style=\"background-color:rgba(135,206,250,0.09883862804500426);\">Central</span> <span style=\"background-color:rgba(135,206,250,0.07515874339938372);\">lead</span> <span style=\"background-color:rgba(135,206,250,0.07829148466786481);\">by</span> <span style=\"background-color:rgba(135,206,250,0.0026846768462767344);\">beating</span> <span style=\"background-color:rgba(135,206,250,0.0037812870178769585);\">the</span> <span style=\"background-color:rgba(135,206,250,0.022921990763560805);\">Minnesota</span> <span style=\"background-color:rgba(135,206,250,0.17904280637096917);\">Twins</span> <span style=\"background-color:rgba(135,206,250,0.018352924272067986);\">,</span> <span style=\"background-color:rgba(135,206,250,0.004801873292181381);\">7</span> <span style=\"background-color:rgba(135,206,250,0.02180551816575541);\">-</span> <span style=\"background-color:rgba(135,206,250,0.08395484660281541);\">1</span> <span style=\"background-color:rgba(135,206,250,0.017956942590752943);\">,</span> <span style=\"background-color:rgba(135,206,250,0.08143760727186322);\">Saturday</span> <span style=\"background-color:rgba(135,206,250,0.04105201784852138);\">night</span> <span style=\"background-color:rgba(135,206,250,0.030618837792303975);\">with</span> <span style=\"background-color:rgba(135,206,250,0.1366647487279042);\">home</span> <span style=\"background-color:rgba(135,206,250,0.020103041417998162);\">runs</span> <span style=\"background-color:rgba(135,206,250,0.06874728265677806);\">by</span> <span style=\"background-color:rgba(135,206,250,0.04433867607246585);\">Travis</span> <span style=\"background-color:rgba(135,206,250,0.29897050608305714);\">Hafner</span> <span style=\"background-color:rgba(135,206,250,0.05268227448579019);\">and</span> <span style=\"background-color:rgba(135,206,250,0.01310628153382018);\">Victor</span> <span style=\"background-color:rgba(135,206,250,0.0381217423478351);\">Martinez</span> <span style=\"background-color:rgba(135,206,250,0.04622789314900899);\">.</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_KMrax3a1Q4"
      },
      "source": [
        "# Checklist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImA_ViRHe4Ul"
      },
      "source": [
        "sample = []\n",
        "for row in test_set.head(10).itertuples():\n",
        "  sample.append(row[3])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "J43GKSRLf7UQ",
        "outputId": "1b4e3dc3-26d8-4ba4-8cd6-3948f1349e9e"
      },
      "source": [
        "for sentence in sample:\n",
        "  result = predictor_with_input_red.predict(sentence)\n",
        "  tokens = result['input_reduction_output']['original']\n",
        "\n",
        "  grad = SimpleGradient(predictor).saliency_interpret_from_json({'Description':sentence})\n",
        "  gradient_list = grad['instance_1']['grad_input_1']\n",
        "  max_alpha = 0.4 \n",
        "  highlighted_text = []\n",
        "  for i in range(len(tokens)):\n",
        "      weight = gradient_list[i]\n",
        "      highlighted_text.append('<span style=\"background-color:rgba(135,206,250,' + str(weight / max_alpha) + ');\">' + html_escape(tokens[i]) + '</span>')\n",
        "  highlighted_text = ' '.join(highlighted_text)\n",
        "  print(display(HTML(highlighted_text)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,0.17663864511266947);\">Unions</span> <span style=\"background-color:rgba(135,206,250,0.005106361665331027);\">representing</span> <span style=\"background-color:rgba(135,206,250,0.28069952232158063);\">workers</span> <span style=\"background-color:rgba(135,206,250,0.1255252336722192);\">at</span> <span style=\"background-color:rgba(135,206,250,0.21222591312691957);\">Turner</span> <span style=\"background-color:rgba(135,206,250,0.39263627187303735);\">Newall</span> <span style=\"background-color:rgba(135,206,250,0.0127987566008682);\">say</span> <span style=\"background-color:rgba(135,206,250,0.011815520268815632);\">they</span> <span style=\"background-color:rgba(135,206,250,0.0258528559077581);\">are</span> <span style=\"background-color:rgba(135,206,250,0.10613540115180552);\">&#x27;</span> <span style=\"background-color:rgba(135,206,250,0.006886830722452546);\">disappointed</span> <span style=\"background-color:rgba(135,206,250,0.03207627621179412);\">&#x27;</span> <span style=\"background-color:rgba(135,206,250,0.08626305733341107);\">after</span> <span style=\"background-color:rgba(135,206,250,0.20752054249945118);\">talks</span> <span style=\"background-color:rgba(135,206,250,0.05260935745912804);\">with</span> <span style=\"background-color:rgba(135,206,250,0.006183001104432847);\">stricken</span> <span style=\"background-color:rgba(135,206,250,0.008669477946375547);\">parent</span> <span style=\"background-color:rgba(135,206,250,0.07372689294589733);\">firm</span> <span style=\"background-color:rgba(135,206,250,0.03373733674431476);\">Federal</span> <span style=\"background-color:rgba(135,206,250,0.5382061518279387);\">Mogul</span> <span style=\"background-color:rgba(135,206,250,0.10468659823894418);\">.</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c577031d1cfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor_with_input_red\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_reduction_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaliency_interpret_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Description'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-39aed32d4ae1>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-ee2ea8e43325>\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, json_dict)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                            \u001b[0minput_field_to_attack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                            \u001b[0mgrad_input_field\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grad_input_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                            ignore_tokens=None)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input_reduction_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/interpret/attackers/input_reduction.py\u001b[0m in \u001b[0;36mattack_from_json\u001b[0;34m(self, inputs, input_field_to_attack, grad_input_field, ignore_tokens, target)\u001b[0m\n\u001b[1;32m     52\u001b[0m             final_tokens.append(\n\u001b[1;32m     53\u001b[0m                 self._attack_instance(\n\u001b[0;32m---> 54\u001b[0;31m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_field_to_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_input_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 )\n\u001b[1;32m     56\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/interpret/attackers/input_reduction.py\u001b[0m in \u001b[0;36m_attack_instance\u001b[0;34m(self, inputs, instance, input_field_to_attack, grad_input_field, ignore_tokens)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;31m# get gradients and predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mbeam_tag_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeam_instance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mhook_layers\u001b[0;34m(module, grad_in, grad_out)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \"\"\"\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mhook_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_offsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyd0aXp4i_Ie"
      },
      "source": [
        "pdata = list(nlp.pipe(sample))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "1MPQ6HELoubj",
        "outputId": "1499c49b-1937-4992-d27c-bfe6ef829bd2"
      },
      "source": [
        "for i in range(10):\n",
        "  sentence = sample[i]\n",
        "  if Perturb.contractions(sample[i]) != []:\n",
        "  \n",
        "    result = predictor_with_input_red.predict(sentence)\n",
        "    tokens = result['input_reduction_output']['original']\n",
        "\n",
        "    grad_orig = SimpleGradient(predictor).saliency_interpret_from_json({'Description':sentence})\n",
        "    gradient_list_orig = np.array(grad_orig['instance_1']['grad_input_1'])\n",
        "\n",
        "    max_alpha = 0.4 \n",
        "    highlighted_text = []\n",
        "    for i in range(len(tokens)):\n",
        "        weight = gradient_list_orig[i]\n",
        "        highlighted_text.append('<span style=\"background-color:rgba(135,206,250,' + str(weight / max_alpha) + ');\">' + html_escape(tokens[i]) + '</span>')\n",
        "    highlighted_text = ' '.join(highlighted_text)\n",
        "    print(display(HTML(highlighted_text)))\n",
        "\n",
        "    perturbed_sentence = Perturb.contractions(sentence)[0]\n",
        "\n",
        "    result = predictor_with_input_red.predict(perturbed_sentence)\n",
        "    tokens = result['input_reduction_output']['original']\n",
        "\n",
        "    grad_pert = SimpleGradient(predictor).saliency_interpret_from_json({'Description': perturbed_sentence})\n",
        "    gradient_list_pert = np.array(grad_pert['instance_1']['grad_input_1'])\n",
        "\n",
        "    max_alpha = 0.4 \n",
        "    highlighted_text = []\n",
        "    for i in range(len(tokens)):\n",
        "        weight = gradient_list_pert[i]\n",
        "        highlighted_text.append('<span style=\"background-color:rgba(135,206,250,' + str(weight / max_alpha) + ');\">' + html_escape(tokens[i]) + '</span>')\n",
        "    highlighted_text = ' '.join(highlighted_text)\n",
        "    print(display(HTML(highlighted_text)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,0.17663864511266947);\">Unions</span> <span style=\"background-color:rgba(135,206,250,0.005106361665331027);\">representing</span> <span style=\"background-color:rgba(135,206,250,0.28069952232158063);\">workers</span> <span style=\"background-color:rgba(135,206,250,0.1255252336722192);\">at</span> <span style=\"background-color:rgba(135,206,250,0.21222591312691957);\">Turner</span> <span style=\"background-color:rgba(135,206,250,0.39263627187303735);\">Newall</span> <span style=\"background-color:rgba(135,206,250,0.0127987566008682);\">say</span> <span style=\"background-color:rgba(135,206,250,0.011815520268815632);\">they</span> <span style=\"background-color:rgba(135,206,250,0.0258528559077581);\">are</span> <span style=\"background-color:rgba(135,206,250,0.10613540115180552);\">&#x27;</span> <span style=\"background-color:rgba(135,206,250,0.006886830722452546);\">disappointed</span> <span style=\"background-color:rgba(135,206,250,0.03207627621179412);\">&#x27;</span> <span style=\"background-color:rgba(135,206,250,0.08626305733341107);\">after</span> <span style=\"background-color:rgba(135,206,250,0.20752054249945118);\">talks</span> <span style=\"background-color:rgba(135,206,250,0.05260935745912804);\">with</span> <span style=\"background-color:rgba(135,206,250,0.006183001104432847);\">stricken</span> <span style=\"background-color:rgba(135,206,250,0.008669477946375547);\">parent</span> <span style=\"background-color:rgba(135,206,250,0.07372689294589733);\">firm</span> <span style=\"background-color:rgba(135,206,250,0.03373733674431476);\">Federal</span> <span style=\"background-color:rgba(135,206,250,0.5382061518279387);\">Mogul</span> <span style=\"background-color:rgba(135,206,250,0.10468659823894418);\">.</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,0.15071278901187968);\">Unions</span> <span style=\"background-color:rgba(135,206,250,0.017807875353869847);\">representing</span> <span style=\"background-color:rgba(135,206,250,0.2553705781894655);\">workers</span> <span style=\"background-color:rgba(135,206,250,0.09268755238276725);\">at</span> <span style=\"background-color:rgba(135,206,250,0.20446270016285717);\">Turner</span> <span style=\"background-color:rgba(135,206,250,0.46992152614494226);\">Newall</span> <span style=\"background-color:rgba(135,206,250,0.01570550693509319);\">say</span> <span style=\"background-color:rgba(135,206,250,0.030333015402822936);\">they</span> <span style=\"background-color:rgba(135,206,250,0.008370800957040243);\">&#x27;re</span> <span style=\"background-color:rgba(135,206,250,0.1296975650135007);\">&#x27;</span> <span style=\"background-color:rgba(135,206,250,0.06103252734551244);\">disappointed</span> <span style=\"background-color:rgba(135,206,250,0.021801175592486708);\">&#x27;</span> <span style=\"background-color:rgba(135,206,250,0.0867486470601403);\">after</span> <span style=\"background-color:rgba(135,206,250,0.19592926484029413);\">talks</span> <span style=\"background-color:rgba(135,206,250,0.06666701915563121);\">with</span> <span style=\"background-color:rgba(135,206,250,0.0037089583834812477);\">stricken</span> <span style=\"background-color:rgba(135,206,250,0.03932823795545208);\">parent</span> <span style=\"background-color:rgba(135,206,250,0.08877841683360833);\">firm</span> <span style=\"background-color:rgba(135,206,250,0.025072038115176824);\">Federal</span> <span style=\"background-color:rgba(135,206,250,0.4203607146760997);\">Mogul</span> <span style=\"background-color:rgba(135,206,250,0.11550325231029261);\">.</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,0.08765118658254974);\">AP</span> <span style=\"background-color:rgba(135,206,250,0.01882906216363848);\">-</span> <span style=\"background-color:rgba(135,206,250,0.04701459214558477);\">It</span> <span style=\"background-color:rgba(135,206,250,0.04780630864871443);\">&#x27;s</span> <span style=\"background-color:rgba(135,206,250,0.020291838739859014);\">barely</span> <span style=\"background-color:rgba(135,206,250,0.25365838139699015);\">dawn</span> <span style=\"background-color:rgba(135,206,250,0.00911937846197057);\">when</span> <span style=\"background-color:rgba(135,206,250,0.016902788794587356);\">Mike</span> <span style=\"background-color:rgba(135,206,250,0.27706057651041854);\">Fitzpatrick</span> <span style=\"background-color:rgba(135,206,250,0.1114817350926789);\">starts</span> <span style=\"background-color:rgba(135,206,250,0.06136877579694456);\">his</span> <span style=\"background-color:rgba(135,206,250,0.1131248714997485);\">shift</span> <span style=\"background-color:rgba(135,206,250,0.0010116360851576318);\">with</span> <span style=\"background-color:rgba(135,206,250,0.00242730902960309);\">a</span> <span style=\"background-color:rgba(135,206,250,0.06583679817484425);\">blur</span> <span style=\"background-color:rgba(135,206,250,0.03499688440720273);\">of</span> <span style=\"background-color:rgba(135,206,250,0.012801938486660457);\">colorful</span> <span style=\"background-color:rgba(135,206,250,0.07611964437713044);\">maps</span> <span style=\"background-color:rgba(135,206,250,0.010628010393309508);\">,</span> <span style=\"background-color:rgba(135,206,250,0.06640727277279765);\">figures</span> <span style=\"background-color:rgba(135,206,250,0.01863659896061983);\">and</span> <span style=\"background-color:rgba(135,206,250,0.04670610214550356);\">endless</span> <span style=\"background-color:rgba(135,206,250,0.09156381160998772);\">charts</span> <span style=\"background-color:rgba(135,206,250,0.005940922746656194);\">,</span> <span style=\"background-color:rgba(135,206,250,0.001993626989029708);\">but</span> <span style=\"background-color:rgba(135,206,250,0.05052024203479925);\">already</span> <span style=\"background-color:rgba(135,206,250,0.003693089946461766);\">he</span> <span style=\"background-color:rgba(135,206,250,0.04461087901431402);\">knows</span> <span style=\"background-color:rgba(135,206,250,0.003981886884081607);\">what</span> <span style=\"background-color:rgba(135,206,250,0.0040168266148105985);\">the</span> <span style=\"background-color:rgba(135,206,250,0.06469106194084319);\">day</span> <span style=\"background-color:rgba(135,206,250,0.03475590077615075);\">will</span> <span style=\"background-color:rgba(135,206,250,0.053232142371572956);\">bring</span> <span style=\"background-color:rgba(135,206,250,0.011658125959483923);\">.</span> <span style=\"background-color:rgba(135,206,250,0.028834156821799205);\">Lightning</span> <span style=\"background-color:rgba(135,206,250,0.03799223651619407);\">will</span> <span style=\"background-color:rgba(135,206,250,0.02578401090980208);\">strike</span> <span style=\"background-color:rgba(135,206,250,0.00043599715745293457);\">in</span> <span style=\"background-color:rgba(135,206,250,0.08362444164295268);\">places</span> <span style=\"background-color:rgba(135,206,250,0.019623887117458366);\">he</span> <span style=\"background-color:rgba(135,206,250,0.00941097564865635);\">expects</span> <span style=\"background-color:rgba(135,206,250,0.013190349433479227);\">.</span> <span style=\"background-color:rgba(135,206,250,0.07011196443771305);\">Winds</span> <span style=\"background-color:rgba(135,206,250,0.0067681401117372355);\">will</span> <span style=\"background-color:rgba(135,206,250,0.08336774874080519);\">pick</span> <span style=\"background-color:rgba(135,206,250,0.007617927087773038);\">up</span> <span style=\"background-color:rgba(135,206,250,0.01198114048679494);\">,</span> <span style=\"background-color:rgba(135,206,250,0.05755341992307272);\">moist</span> <span style=\"background-color:rgba(135,206,250,0.09674878906776765);\">places</span> <span style=\"background-color:rgba(135,206,250,0.00896761409449664);\">will</span> <span style=\"background-color:rgba(135,206,250,0.05408936824139431);\">dry</span> <span style=\"background-color:rgba(135,206,250,0.011053822478371353);\">and</span> <span style=\"background-color:rgba(135,206,250,0.042391000642147514);\">flames</span> <span style=\"background-color:rgba(135,206,250,0.02251052756189667);\">will</span> <span style=\"background-color:rgba(135,206,250,0.031248189000046264);\">roar</span> <span style=\"background-color:rgba(135,206,250,0.006154040264765579);\">.</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,0.0743132511420831);\">AP</span> <span style=\"background-color:rgba(135,206,250,0.018902214276547955);\">-</span> <span style=\"background-color:rgba(135,206,250,0.08576494383945778);\">It</span> <span style=\"background-color:rgba(135,206,250,0.0717503977822672);\">is</span> <span style=\"background-color:rgba(135,206,250,0.0124227877028508);\">barely</span> <span style=\"background-color:rgba(135,206,250,0.23707897836649638);\">dawn</span> <span style=\"background-color:rgba(135,206,250,0.002097988052037242);\">when</span> <span style=\"background-color:rgba(135,206,250,0.031812292369453855);\">Mike</span> <span style=\"background-color:rgba(135,206,250,0.26463512361907726);\">Fitzpatrick</span> <span style=\"background-color:rgba(135,206,250,0.10916675157043594);\">starts</span> <span style=\"background-color:rgba(135,206,250,0.05941426850005063);\">his</span> <span style=\"background-color:rgba(135,206,250,0.11196361334295282);\">shift</span> <span style=\"background-color:rgba(135,206,250,0.0005247716297463262);\">with</span> <span style=\"background-color:rgba(135,206,250,0.002635146364299701);\">a</span> <span style=\"background-color:rgba(135,206,250,0.07014935760279199);\">blur</span> <span style=\"background-color:rgba(135,206,250,0.03512874631511271);\">of</span> <span style=\"background-color:rgba(135,206,250,0.014680388737409598);\">colorful</span> <span style=\"background-color:rgba(135,206,250,0.07151813768643878);\">maps</span> <span style=\"background-color:rgba(135,206,250,0.008874195147667924);\">,</span> <span style=\"background-color:rgba(135,206,250,0.07085538442330197);\">figures</span> <span style=\"background-color:rgba(135,206,250,0.016676138598638847);\">and</span> <span style=\"background-color:rgba(135,206,250,0.04428960909823927);\">endless</span> <span style=\"background-color:rgba(135,206,250,0.08202832917061618);\">charts</span> <span style=\"background-color:rgba(135,206,250,0.005827195789128643);\">,</span> <span style=\"background-color:rgba(135,206,250,0.001220173997848697);\">but</span> <span style=\"background-color:rgba(135,206,250,0.05002735974737086);\">already</span> <span style=\"background-color:rgba(135,206,250,0.002672191712488061);\">he</span> <span style=\"background-color:rgba(135,206,250,0.043367874181857204);\">knows</span> <span style=\"background-color:rgba(135,206,250,0.004613938535972221);\">what</span> <span style=\"background-color:rgba(135,206,250,0.003970012595895506);\">the</span> <span style=\"background-color:rgba(135,206,250,0.06576057187449244);\">day</span> <span style=\"background-color:rgba(135,206,250,0.03678874060825894);\">will</span> <span style=\"background-color:rgba(135,206,250,0.04461237173999931);\">bring</span> <span style=\"background-color:rgba(135,206,250,0.013609229738757935);\">.</span> <span style=\"background-color:rgba(135,206,250,0.0465279895063111);\">Lightning</span> <span style=\"background-color:rgba(135,206,250,0.03545991119401642);\">will</span> <span style=\"background-color:rgba(135,206,250,0.019635532757802762);\">strike</span> <span style=\"background-color:rgba(135,206,250,0.00033898734132362937);\">in</span> <span style=\"background-color:rgba(135,206,250,0.08516262695864482);\">places</span> <span style=\"background-color:rgba(135,206,250,0.01860040293209202);\">he</span> <span style=\"background-color:rgba(135,206,250,0.00048247844648897665);\">expects</span> <span style=\"background-color:rgba(135,206,250,0.013438464788576758);\">.</span> <span style=\"background-color:rgba(135,206,250,0.06499785506495694);\">Winds</span> <span style=\"background-color:rgba(135,206,250,0.0064983166759927605);\">will</span> <span style=\"background-color:rgba(135,206,250,0.08403241611432304);\">pick</span> <span style=\"background-color:rgba(135,206,250,0.006255416687458244);\">up</span> <span style=\"background-color:rgba(135,206,250,0.012468853409142323);\">,</span> <span style=\"background-color:rgba(135,206,250,0.053522473216914855);\">moist</span> <span style=\"background-color:rgba(135,206,250,0.09807206300661227);\">places</span> <span style=\"background-color:rgba(135,206,250,0.008410745257139926);\">will</span> <span style=\"background-color:rgba(135,206,250,0.05685133369690216);\">dry</span> <span style=\"background-color:rgba(135,206,250,0.009489261303500581);\">and</span> <span style=\"background-color:rgba(135,206,250,0.04492121571622283);\">flames</span> <span style=\"background-color:rgba(135,206,250,0.023660181274006886);\">will</span> <span style=\"background-color:rgba(135,206,250,0.041649056084372055);\">roar</span> <span style=\"background-color:rgba(135,206,250,0.00037199225184837703);\">.</span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-7f92b1f09153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mPerturb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor_with_input_red\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_reduction_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-39aed32d4ae1>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-ee2ea8e43325>\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, json_dict)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                            \u001b[0minput_field_to_attack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                            \u001b[0mgrad_input_field\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grad_input_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                            ignore_tokens=None)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input_reduction_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/interpret/attackers/input_reduction.py\u001b[0m in \u001b[0;36mattack_from_json\u001b[0;34m(self, inputs, input_field_to_attack, grad_input_field, ignore_tokens, target)\u001b[0m\n\u001b[1;32m     52\u001b[0m             final_tokens.append(\n\u001b[1;32m     53\u001b[0m                 self._attack_instance(\n\u001b[0;32m---> 54\u001b[0;31m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_field_to_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_input_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 )\n\u001b[1;32m     56\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/interpret/attackers/input_reduction.py\u001b[0m in \u001b[0;36m_attack_instance\u001b[0;34m(self, inputs, instance, input_field_to_attack, grad_input_field, ignore_tokens)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;31m# get gradients and predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mbeam_tag_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeam_instance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mhook_layers\u001b[0;34m(module, grad_in, grad_out)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \"\"\"\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mhook_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_offsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj8tzhz95HfE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}